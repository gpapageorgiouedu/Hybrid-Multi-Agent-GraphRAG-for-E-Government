{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjaZw4RnWKsL"
      },
      "source": [
        "#### Pip and installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwCDzjVL_Qdk"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/gpapageorgiouedu/Hybrid-Multi-Agent-GraphRAG-for-E-Government.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPwLo_NrOuLA"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "neo4j-haystack==2.2.1 \\\n",
        "openai==1.72.0 \\\n",
        "sentence-transformers==3.4.1 \\\n",
        "yfiles_jupyter_graphs==1.10.2 \\\n",
        "trafilatura==2.0.0 \\\n",
        "fastapi==0.115.12 \\\n",
        "uvicorn==0.34.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TCf9E-_Blak"
      },
      "outputs": [],
      "source": [
        "# core libs imports for data handling, file management, and type annotations\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# google colab utils for output and secure data storage\n",
        "from google.colab import output, userdata\n",
        "from google.colab.output import eval_js\n",
        "from google.colab import files\n",
        "\n",
        "# neo4j database integration\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# haystack/ neo4j integration components\n",
        "from neo4j_haystack import Neo4jDocumentStore, Neo4jEmbeddingRetriever\n",
        "\n",
        "# graph visualization in notebooks (explanatory)\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "\n",
        "# haystack components for pipeline construction and data processing\n",
        "from haystack import Pipeline, component, Document\n",
        "from haystack.core.component import component, Component\n",
        "from haystack.components.agents import Agent\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.preprocessors import DocumentCleaner\n",
        "from haystack.components.rankers import TransformersSimilarityRanker\n",
        "from haystack.components.websearch import SerperDevWebSearch\n",
        "from haystack.components.evaluators import FaithfulnessEvaluator, ContextRelevanceEvaluator\n",
        "from haystack.core import SuperComponent\n",
        "from haystack.dataclasses import ChatMessage, ToolCall, ToolCallResult, TextContent\n",
        "from haystack.tools.component_tool import ComponentTool\n",
        "from haystack.utils import Secret\n",
        "\n",
        "# fastapi framework for building backend APIs\n",
        "from fastapi import FastAPI, HTTPException, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# server and threading utilities\n",
        "from threading import Thread\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjVf-8QzOuPo"
      },
      "outputs": [],
      "source": [
        "# set environment variables from secure colab userdata and read environment variables into local constants\n",
        "os.environ[\"NEO4J_URI\"] = userdata.get(\"NEO4J_URI\")\n",
        "os.environ[\"NEO4J_USERNAME\"] = userdata.get(\"NEO4J_USERNAME\")\n",
        "os.environ[\"NEO4J_PASSWORD\"] = userdata.get(\"NEO4J_PASSWORD\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI\")\n",
        "os.environ[\"SERPERDEV_API_KEY\"] = userdata.get(\"SERPER\")\n",
        "\n",
        "NEO4J_URI = os.environ[\"NEO4J_URI\"]\n",
        "NEO4J_USER = os.environ[\"NEO4J_USERNAME\"]\n",
        "NEO4J_PASS = os.environ[\"NEO4J_PASSWORD\"]\n",
        "SERPERDEV_API_KEY = os.environ[\"SERPERDEV_API_KEY\"]\n",
        "\n",
        "# init and test neo4j connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        info = session.run(\"RETURN 1 AS result\").single()\n",
        "        print(\"Neo4j connected, test query result:\", info[\"result\"])\n",
        "finally:\n",
        "    driver.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIAcD3G4C5xM"
      },
      "source": [
        "#### Delete Docs (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijiLK5kWCpWz"
      },
      "outputs": [],
      "source": [
        "def delete_all(tx):\n",
        "    \"\"\"\n",
        "    Delete all nodes and relationships from the graph.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction object.\n",
        "    \"\"\"\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "\n",
        "def count_remaining(tx):\n",
        "    \"\"\"\n",
        "    Count remaining nodes in the graph.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction object.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of nodes remaining.\n",
        "    \"\"\"\n",
        "    result = tx.run(\"MATCH (n) RETURN count(n) AS node_count\")\n",
        "    return result.single()[\"node_count\"]\n",
        "\n",
        "\n",
        "def list_node_labels(tx):\n",
        "    \"\"\"\n",
        "    List all unique labels in the graph.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction object.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of label names.\n",
        "    \"\"\"\n",
        "    result = tx.run(\"CALL db.labels()\")\n",
        "    return [record[\"label\"] for record in result]\n",
        "\n",
        "\n",
        "def count_by_label(tx, label):\n",
        "    \"\"\"\n",
        "    Count the number of nodes for a specific label.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction object.\n",
        "        label (str): The label to count.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of nodes with the given label.\n",
        "    \"\"\"\n",
        "    result = tx.run(f\"MATCH (n:`{label}`) RETURN count(n) AS count\")\n",
        "    return result.single()[\"count\"]\n",
        "\n",
        "\n",
        "# display node counts grouped by label\n",
        "with driver.session() as session:\n",
        "    labels = session.read_transaction(list_node_labels)\n",
        "    for label in labels:\n",
        "        count = session.read_transaction(count_by_label, label)\n",
        "        print(f\"Label: {label}, Count: {count}\")\n",
        "\n",
        "\n",
        "def force_delete_by_labels(tx, labels):\n",
        "    \"\"\"\n",
        "    Forcefully delete nodes by specified labels.\n",
        "\n",
        "    Args:\n",
        "        tx: Neo4j transaction object.\n",
        "        labels (List[str]): A list of node labels to delete.\n",
        "    \"\"\"\n",
        "    for label in labels:\n",
        "        tx.run(f\"MATCH (n:`{label}`) DETACH DELETE n\")\n",
        "\n",
        "\n",
        "# delete all nodes by label, and verify complete deletion\n",
        "with driver.session() as session:\n",
        "    labels = session.read_transaction(list_node_labels)\n",
        "    session.write_transaction(force_delete_by_labels, labels)\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.write_transaction(delete_all)\n",
        "    remaining = session.read_transaction(count_remaining)\n",
        "\n",
        "    if remaining == 0:\n",
        "        print(\"All nodes and relationships successfully deleted.\")\n",
        "    else:\n",
        "        print(f\"Deletion incomplete: {remaining} node(s) still exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giPAmjbuC_om"
      },
      "source": [
        "#### Index Docs (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XPHYIsaOuWt"
      },
      "outputs": [],
      "source": [
        "def load_json_documents(json_folder_path):\n",
        "    \"\"\"\n",
        "    Load JSON files from a folder and convert them into Haystack Document objects.\n",
        "\n",
        "    Args:\n",
        "        json_folder_path (str or Path): Path to the folder containing .json files.\n",
        "\n",
        "    Returns:\n",
        "        List[Document]: A list of Haystack Document objects with content and metadata.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for json_file in Path(json_folder_path).glob(\"*.json\"):\n",
        "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "            # standardize metadata keys -> add any fits to your data use case\n",
        "            if \"url\" in data:\n",
        "                data[\"source_url\"] = data.pop(\"source_url\")\n",
        "            if \"date\" in data:\n",
        "                data[\"date\"] = data.pop(\"date\")\n",
        "            if \"title\" in data:\n",
        "                data[\"title\"] = data.pop(\"title\")\n",
        "\n",
        "            content = data.get(\"content\", \"\")\n",
        "            metadata = {k: v for k, v in data.items() if k != \"content\"}\n",
        "\n",
        "            documents.append(Document(content=content, meta=metadata))\n",
        "    return documents\n",
        "\n",
        "\n",
        "# load raw documents from a local folder\n",
        "raw_docs = load_json_documents(\"json_docs\")\n",
        "\n",
        "# clean the documents before embedding\n",
        "cleaner = DocumentCleaner(\n",
        "    remove_empty_lines=True,\n",
        "    remove_extra_whitespaces=True,\n",
        "    remove_substrings=[\"...\"]\n",
        ")\n",
        "cleaned_docs = cleaner.run(raw_docs)[\"documents\"]\n",
        "\n",
        "# init neo4j document store for embedding storage\n",
        "document_store = Neo4jDocumentStore(\n",
        "    url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASS,\n",
        "    database=\"neo4j\",\n",
        "    index=\"document-embeddings\",\n",
        "    embedding_field=\"embedding\",\n",
        "    embedding_dim=1536,\n",
        "    node_label=\"Document\"\n",
        ")\n",
        "\n",
        "# generate embeddings (ada-002 in our use case)\n",
        "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-ada-002\")\n",
        "documents_with_emb = embedder.run(cleaned_docs)[\"documents\"]\n",
        "\n",
        "# index the embedded documents in the neo4j store\n",
        "document_store.write_documents(documents_with_emb)\n",
        "print(f\"Indexed {document_store.count_documents()} documents in Neo4j.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlUCCyVoOubM"
      },
      "outputs": [],
      "source": [
        "def extract_structured_triples(text_chunk):\n",
        "    \"\"\"\n",
        "    Extract structured knowledge triples from a given text using an OpenAI language model.\n",
        "\n",
        "    The output is a list of dictionaries, where each dictionary represents a triple with:\n",
        "    - 'head': subject of the triple\n",
        "    - 'head_type': type/classification of the head\n",
        "    - 'relation': relationship between head and tail (in UPPER_SNAKE_CASE)\n",
        "    - 'tail': object of the triple\n",
        "    - 'tail_type': type/classification of the tail\n",
        "\n",
        "    Args:\n",
        "        text_chunk (str): A passage of text from which to extract triples.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, str]]: A list of structured triples, or an empty list on failure.\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are an information extraction assistant.\"\n",
        "        \"You are an expert in European Union's news, policies, laws, and actions. \"\n",
        "        \"Extract all factual knowledge triples from the text in a structured format. \"\n",
        "        \"Return the results as a JSON list where each item is an object with the keys: \"\n",
        "        \"'head', 'head_type', 'relation', 'tail', 'tail_type'.\\n\\n\"\n",
        "        \"Guidelines:\\n\"\n",
        "        \"Resolve vague pronouns (like 'I', 'we', 'they', 'he/she') to actual entities based on context.\\n\"\n",
        "        \"Use the standard full format, even when abbreviations are used in the text. For example, when 'EU' is used, write it as 'European Union'.\"\n",
        "        \"Use the standard full format for names, even if the full name is not used entirely in a specific sentence.\"\n",
        "        \"Include the full context in the extracted triples to ensure they are informative and comprehensive.\\n\"\n",
        "        \"Maintain consistency: refer to entities by their full and most complete identifiers.\\n\"\n",
        "        \"Use concise relation phrases written in UPPER_SNAKE_CASE.\\n\"\n",
        "        \"Avoid vague, incomplete, or uninformative triples. Use full context to provide informative and comprehensive triples.\\n\"\n",
        "        \"Return only the JSON list of objects. Do not include any explanations, additional knowledge, or markdown.\\n\"\n",
        "        \"If an entity type is unclear, make a reasonable guess or use a general type like 'Entity'.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"Text: ```{text_chunk}```\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content.strip()\n",
        "    content = re.sub(r\"^```json\\n?\", \"\", content)\n",
        "    content = re.sub(r\"\\n?```$\", \"\", content)\n",
        "\n",
        "    try:\n",
        "        structured_triples = json.loads(content)\n",
        "    except json.JSONDecodeError as json_err:\n",
        "        print(\"JSON decoding error:\", json_err)\n",
        "        print(\"Raw output was:\\n\", content[:500])\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return []\n",
        "\n",
        "    return structured_triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0tHgyvwOudM"
      },
      "outputs": [],
      "source": [
        "def sanitize_label(label):\n",
        "    \"\"\"\n",
        "    Sanitize a string to be a valid Neo4j label.\n",
        "\n",
        "    Converts non-alphanumeric characters to underscores and ensures the first letter is uppercase.\n",
        "    Returns a default value if the label is empty after sanitization.\n",
        "\n",
        "    Args:\n",
        "        label (str): The label string to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: A sanitized, Neo4j-safe label.\n",
        "    \"\"\"\n",
        "    label = re.sub(r\"[^a-zA-Z0-9]\", \"_\", label.strip())\n",
        "    if not label:\n",
        "        return \"Entity\"\n",
        "    return label[0].upper() + label[1:]\n",
        "\n",
        "\n",
        "# ensure a full-text index exists for entity IDs\n",
        "with driver.session() as session:\n",
        "    try:\n",
        "        session.run(\"CREATE FULLTEXT INDEX entity_index IF NOT EXISTS FOR (n) ON EACH [n.id]\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# extract triples and insert them into neo4j with typed nodes and relationships\n",
        "with driver.session() as session:\n",
        "    for doc in documents_with_emb:\n",
        "        text = doc.content\n",
        "        triples = extract_structured_triples(text)\n",
        "        print(len(triples))\n",
        "        doc_id = doc.id\n",
        "\n",
        "        for triple in triples:\n",
        "            subj = triple.get(\"head\")\n",
        "            subj_type = sanitize_label(triple.get(\"head_type\", \"Entity\"))\n",
        "            pred = triple.get(\"relation\")\n",
        "            obj = triple.get(\"tail\")\n",
        "            obj_type = sanitize_label(triple.get(\"tail_type\", \"Entity\"))\n",
        "\n",
        "            if not subj or not pred or not obj:\n",
        "                continue\n",
        "\n",
        "            rel_type = \"_\".join(pred.strip().split()).upper()\n",
        "            rel_type = re.sub(r\"[^A-Z0-9_]\", \"_\", rel_type)\n",
        "\n",
        "            cypher = f\"\"\"\n",
        "            MERGE (s:{subj_type} {{id: $subj}})\n",
        "            MERGE (o:{obj_type} {{id: $obj}})\n",
        "            MERGE (s)-[r:{rel_type}]->(o)\n",
        "            MERGE (d:Document {{id: $doc_id}})\n",
        "            MERGE (d)-[:MENTIONS]->(s)\n",
        "            MERGE (d)-[:MENTIONS]->(o)\n",
        "            \"\"\"\n",
        "\n",
        "            session.run(cypher, {\n",
        "                \"subj\": subj,\n",
        "                \"obj\": obj,\n",
        "                \"doc_id\": doc_id\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBA-fAZDKtu"
      },
      "source": [
        "#### Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pHc-aFuh-EK"
      },
      "outputs": [],
      "source": [
        "@component\n",
        "class KnowledgeGraphRetriever():\n",
        "    \"\"\"\n",
        "    A custom Haystack component for retrieving context-rich documents from a Neo4j knowledge graph\n",
        "    based on search terms extracted by an OpenAI model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_pass: str, openai_model=\"gpt-4.1-mini\"):\n",
        "        self._driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_pass))\n",
        "        self._model = openai_model\n",
        "\n",
        "    @component.output_types(documents=List[Document])\n",
        "    def run(self, query: str) -> Dict[str, List[Document]]:\n",
        "        \"\"\"\n",
        "        Run retrieval based on an input query.\n",
        "\n",
        "        Uses a language model to extract search terms, runs Cypher queries against Neo4j,\n",
        "        and formats the results into Haystack Document objects.\n",
        "\n",
        "        Args:\n",
        "            query (str): The natural language query from the user.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Document]]: A dictionary with a single key \"documents\" containing the result set.\n",
        "        \"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are a search term extractor. Based on the user's question, return a list of 1–3 keywords or named entities \"\n",
        "            \"that should be used to search a knowledge graph. Use lowercase, and return only a clean list in JSON like [\\\"term1\\\", \\\"term2\\\"]\"\n",
        "        )\n",
        "\n",
        "        response = openai.chat.completions.create(\n",
        "            model=self._model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        raw_content = response.choices[0].message.content.strip()\n",
        "        try:\n",
        "            terms = re.findall(r'\"(.*?)\"', raw_content)\n",
        "            if not terms:\n",
        "                terms = [query]\n",
        "        except Exception:\n",
        "            terms = [query]\n",
        "\n",
        "        documents = []\n",
        "\n",
        "        with self._driver.session() as session:\n",
        "            for term in terms:\n",
        "                cypher = \"\"\"\n",
        "                    MATCH (n)-[r]-(connected)\n",
        "                    WHERE toLower(n.id) CONTAINS toLower($query)\n",
        "                    OPTIONAL MATCH (n)<-[:MENTIONS]-(d:Document)\n",
        "                    OPTIONAL MATCH (connected)<-[:MENTIONS]-(d2:Document)\n",
        "                    RETURN n, r, connected, coalesce(d, d2) AS doc\n",
        "                \"\"\"\n",
        "                result = session.run(cypher, {\"query\": term})\n",
        "\n",
        "                grouped_output = defaultdict(lambda: {\"to_doc\": [], \"other\": []})\n",
        "                doc_text_lookup = {}\n",
        "\n",
        "                for record in result:\n",
        "                    n = record[\"n\"]\n",
        "                    r = record[\"r\"]\n",
        "                    connected = record[\"connected\"]\n",
        "                    doc_node = record.get(\"doc\")\n",
        "\n",
        "                    n_label = list(n.labels)[0] if n.labels else \"Entity\"\n",
        "                    connected_label = list(connected.labels)[0] if connected.labels else \"Entity\"\n",
        "                    n_id = n.get(\"id\", \"[no-id]\")\n",
        "\n",
        "                    if doc_node:\n",
        "                        doc_id = doc_node.get(\"id\", \"unknown\")\n",
        "                        doc_content = doc_node.get(\"content\", \"[No content]\")\n",
        "                        doc_title = doc_node.get(\"title\", \"[No Title]\")\n",
        "                        doc_url = doc_node.get(\"source_url\", \"[No URL]\")\n",
        "                        doc_date = doc_node.get(\"date\", \"[No Date]\")\n",
        "                        full_doc_text = f\"Title: {doc_title}\\nDate: {doc_date}\\nURL: {doc_url}\\n\\n{doc_content}\"\n",
        "                        doc_text_lookup[doc_id] = full_doc_text\n",
        "                    else:\n",
        "                        doc_id = \"no_doc\"\n",
        "\n",
        "                    is_connected_doc = \"Document\" in connected.labels\n",
        "\n",
        "                    if is_connected_doc:\n",
        "                        triple_line = f\"({n_label}: {n_id}) -[{r.type}]-> In Document below:\"\n",
        "                        grouped_output[doc_id][\"to_doc\"].append(triple_line)\n",
        "                    else:\n",
        "                        connected_value = connected.get(\"id\", \"[no-id]\")\n",
        "                        triple_line = f\"({n_label}: {n_id}) -[{r.type}]-> ({connected_label}: {connected_value})\"\n",
        "                        grouped_output[doc_id][\"other\"].append(triple_line)\n",
        "\n",
        "                for doc_id, groups in grouped_output.items():\n",
        "                    doc_lines = groups[\"to_doc\"]\n",
        "                    other_lines = groups[\"other\"]\n",
        "                    content_parts = []\n",
        "\n",
        "                    if doc_lines:\n",
        "                        content_parts.extend(doc_lines)\n",
        "                        doc_text = doc_text_lookup.get(doc_id, \"[No document content]\")\n",
        "                        content_parts.append(\"\\nDocument:\\n\" + doc_text)\n",
        "\n",
        "                    if other_lines:\n",
        "                        content_parts.append(\"\")\n",
        "                        content_parts.extend(other_lines)\n",
        "\n",
        "                    final_content = \"\\n\".join(content_parts).strip()\n",
        "                    meta = {\"source_doc_id\": doc_id} if doc_id != \"no_doc\" else {}\n",
        "                    documents.append(Document(content=final_content, meta=meta))\n",
        "\n",
        "        if not documents:\n",
        "            documents.append(Document(content=\"(No results found)\"))\n",
        "\n",
        "        return {\"documents\": documents}\n",
        "\n",
        "@component\n",
        "class DocumentPassthrough:\n",
        "    def run(self, documents: List[Document]) -> Dict[str, List[Document]]:\n",
        "        return {\"documents\": documents}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsPnXC9l-pM4"
      },
      "outputs": [],
      "source": [
        "# config the neo4j document store for retrieval\n",
        "document_store = Neo4jDocumentStore(\n",
        "    url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PASS,\n",
        "    database=\"neo4j\",\n",
        "    index=\"document-embeddings\",\n",
        "    embedding_field=\"embedding\",\n",
        "    embedding_dim=1536,\n",
        "    node_label=\"Document\"\n",
        ")\n",
        "\n",
        "# init components\n",
        "embedder_emb = OpenAITextEmbedder(model=\"text-embedding-ada-002\")\n",
        "\n",
        "retriever_emb = Neo4jEmbeddingRetriever(document_store=document_store)\n",
        "\n",
        "ranker_emb = TransformersSimilarityRanker(\n",
        "    model=\"intfloat/simlm-msmarco-reranker\", top_k=5\n",
        ")\n",
        "\n",
        "prompt_template_emb = \"\"\"\n",
        "You are an AI Assistant with access to official documents about the European Union's news, policies, laws, and actions.\n",
        "Your task is to answer user questions strictly based on the documents provided below.\n",
        "\n",
        "Guidelines:\n",
        "Use only the content from the provided Documents.\n",
        "Do NOT rely on prior or external knowledge.\n",
        "\n",
        "Do NOT ask the user for additional information.\n",
        "\n",
        "Include inline HTML links for referencing URL sources in the answer, using the URLs provided in the Documents.\n",
        "Use the document’s title as the anchor text. If the title is missing, use the domain name of the document’s URL as the anchor text.\n",
        "Each fact you refer to should be followed by the corresponding reference.\n",
        "\n",
        "Output the answer in a structured markdown format.\n",
        "Use bullet lists whenever it makes sense.\n",
        "Do not add a references section at the end of the answer, just use references within the body of text.\n",
        "\n",
        "If a definitive answer cannot be found in the Documents, respond with:\n",
        "Final Answer: inconclusive\n",
        "\n",
        "Always end your answer with this disclaimer:\n",
        "Disclaimer: This is AI generated content — please use it with caution.\n",
        "\n",
        "Documents:\n",
        "{% for doc in documents %}\n",
        "Source: <a href=\"{{ doc.meta.source_url }}\"</a><br>\n",
        "Title: <a href=\"{{ doc.meta.title }}\"</a><br>\n",
        "Date: <a href=\"{{ doc.meta.date }}\"</a><br>\n",
        "\n",
        "{{ doc.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ query }}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_emb = PromptBuilder(\n",
        "    template=prompt_template_emb,\n",
        "    required_variables=[\"documents\", \"query\"]\n",
        ")\n",
        "\n",
        "generator_emb = OpenAIGenerator(\n",
        "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# create the pipeline, register components and connect pipeline components\n",
        "emb_pipeline = Pipeline()\n",
        "emb_pipeline.add_component(\"embedder\", embedder_emb)\n",
        "emb_pipeline.add_component(\"retriever\", retriever_emb)\n",
        "emb_pipeline.add_component(\"reranker\", ranker_emb)\n",
        "emb_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
        "emb_pipeline.add_component(\"prompt_builder\", prompt_builder_emb)\n",
        "emb_pipeline.add_component(\"generator\", generator_emb)\n",
        "\n",
        "emb_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "emb_pipeline.connect(\"retriever.documents\", \"reranker.documents\")\n",
        "emb_pipeline.connect(\"reranker.documents\", \"output_docs.documents\")\n",
        "emb_pipeline.connect(\"reranker.documents\", \"prompt_builder.documents\")\n",
        "emb_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "# prepare pipeline for use\n",
        "emb_pipeline.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7532GE2FXQj"
      },
      "outputs": [],
      "source": [
        "# define components for web based retrieval and generation\n",
        "web_search = SerperDevWebSearch(top_k=5, api_key=Secret.from_env_var(\"SERPERDEV_API_KEY\"))\n",
        "fetcher = LinkContentFetcher()\n",
        "converter = HTMLToDocument()\n",
        "ranker_web = TransformersSimilarityRanker(model=\"intfloat/simlm-msmarco-reranker\", top_k=5)\n",
        "\n",
        "# prompt template with required variables for generation\n",
        "prompt_template_web = \"\"\"\n",
        "You are a helpful AI assistant. Use only the web search documents below to answer the user’s question.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Use only the content from the provided documents.\n",
        "Do NOT use prior knowledge or make assumptions.\n",
        "\n",
        "When referencing a source, include an inline HTML link using the document’s URL.\n",
        "Each fact you refer to should be followed by the corresponding reference.\n",
        "If a clear title can be inferred from the document content or URL, you may use it as the anchor text.\n",
        "If no clear title can be inferred, use the domain name of the URL as the anchor text.\n",
        "\n",
        "Output the answer in a structured markdown format.\n",
        "Use bullet lists whenever it makes sense.\n",
        "Do not add a references section at the end of the answer, just use references within the body of text.\n",
        "\n",
        "If no relevant information is found in the documents, respond with:\n",
        "Final Answer: inconclusive\n",
        "\n",
        "Always end your answer with this disclaimer:\n",
        "Disclaimer: This is AI generated content — please use it with caution.\n",
        "\n",
        "Documents:\n",
        "{% for doc in documents %}\n",
        "- <b>Source:</b> <a href=\"{{ doc.meta.url }}\">{{ doc.meta.url }}</a><br>\n",
        "<p>{{ doc.content }}</p><br>\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ query }}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_web = PromptBuilder(\n",
        "    template=prompt_template_web,\n",
        "    required_variables=[\"documents\", \"query\"]\n",
        ")\n",
        "\n",
        "generator_web = OpenAIGenerator(\n",
        "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# create, config the web search pipeline and connect the components\n",
        "web_pipeline = Pipeline()\n",
        "web_pipeline.add_component(\"search\", web_search)\n",
        "web_pipeline.add_component(\"fetcher\", fetcher)\n",
        "web_pipeline.add_component(\"converter\", converter)\n",
        "web_pipeline.add_component(\"ranker\", ranker_web)\n",
        "web_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
        "web_pipeline.add_component(\"prompt_builder\", prompt_builder_web)\n",
        "web_pipeline.add_component(\"generator\", generator_web)\n",
        "\n",
        "web_pipeline.connect(\"search.links\", \"fetcher.urls\")\n",
        "web_pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "web_pipeline.connect(\"converter.documents\", \"ranker.documents\")\n",
        "web_pipeline.connect(\"ranker.documents\", \"output_docs.documents\")\n",
        "web_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
        "web_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "# prepare the pipeline for inference\n",
        "web_pipeline.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJDGVqoXNRly"
      },
      "outputs": [],
      "source": [
        "# init the knowledge graph retriever component\n",
        "kg_retriever = KnowledgeGraphRetriever(\n",
        "    neo4j_uri=NEO4J_URI,\n",
        "    neo4j_user=NEO4J_USER,\n",
        "    neo4j_pass=NEO4J_PASS\n",
        ")\n",
        "\n",
        "# Set up reranker for refining retrieved graph-based documents\n",
        "ranker_graph = TransformersSimilarityRanker(\n",
        "    model=\"intfloat/simlm-msmarco-reranker\", top_k=5\n",
        ")\n",
        "\n",
        "# Define prompt template tailored for graph-based documents\n",
        "prompt_template_graph = \"\"\"\n",
        "You are a helpful AI assistant working with structured information derived from a knowledge graph.\n",
        "\n",
        "Use only the provided documents below to answer the user's question.\n",
        "Each document contains factual relationships (triples) extracted from a graph, along with the original source content from which the relationships were derived.\n",
        "\n",
        "Focus specifically on topics related to the European Union's news, policies, laws, and actions.\n",
        "\n",
        "Instructions:\n",
        "Base your answer strictly on the information in the documents.\n",
        "\n",
        "Do NOT use external knowledge or assumptions.\n",
        "\n",
        "When referencing a source, include an inline HTML link using the document’s title as the anchor text.\n",
        "If a title cannot be inferred, use the domain name of the document’s URL as the anchor text.\n",
        "Each fact you refer to should be followed by the corresponding reference.\n",
        "\n",
        "Output the answer in a structured markdown format.\n",
        "Use bullet lists whenever it makes sense.\n",
        "Do not add a references section at the end of the answer, just use references within the body of text.\n",
        "\n",
        "If no relevant information is found in the documents, respond with:\n",
        "Final Answer: inconclusive\n",
        "\n",
        "Always end your answer with this disclaimer:\n",
        "Disclaimer: This is AI generated content — please use it with caution.\n",
        "\n",
        "Documents:\n",
        "{% for doc in documents %}\n",
        "- <b>Source:</b><br>\n",
        "{{ doc.content }}<br><br>\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ query }}\n",
        "\n",
        "Answer (with references using HTML links):\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder_graph = PromptBuilder(\n",
        "    template=prompt_template_graph,\n",
        "    required_variables=[\"documents\", \"query\"]\n",
        ")\n",
        "\n",
        "# init generator\n",
        "generator_graph = OpenAIGenerator(\n",
        "    model=\"gpt-4.1-mini\", api_key=Secret.from_env_var(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Build the knowledge graph pipeline\n",
        "graph_pipeline = Pipeline()\n",
        "graph_pipeline.add_component(\"kg_retriever\", kg_retriever)\n",
        "graph_pipeline.add_component(\"ranker\", ranker_graph)\n",
        "graph_pipeline.add_component(\"output_docs\", DocumentPassthrough())\n",
        "graph_pipeline.add_component(\"prompt_builder\", prompt_builder_graph)\n",
        "graph_pipeline.add_component(\"generator\", generator_graph)\n",
        "\n",
        "# Define the flow of data between components\n",
        "graph_pipeline.connect(\"kg_retriever.documents\", \"ranker.documents\")\n",
        "graph_pipeline.connect(\"ranker.documents\", \"output_docs.documents\")\n",
        "graph_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
        "graph_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "# Prepare the pipeline\n",
        "graph_pipeline.warm_up()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyJjq0u0QJt0"
      },
      "source": [
        "#### Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbuFqXdcNRp1"
      },
      "outputs": [],
      "source": [
        "# init the chat generator for multi-turn interaction\n",
        "chat_generator = OpenAIChatGenerator(\n",
        "    model=\"gpt-4.1\",\n",
        "    api_key=Secret.from_env_var(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# wrap the pipelines with input/output mappings\n",
        "embedding_super = SuperComponent(\n",
        "    pipeline=emb_pipeline,\n",
        "    input_mapping={\n",
        "        \"query\": [\"embedder.text\", \"reranker.query\", \"prompt_builder.query\"]\n",
        "    },\n",
        "    output_mapping={\"generator.replies\": \"replies\"}\n",
        ")\n",
        "\n",
        "graph_super = SuperComponent(\n",
        "    pipeline=graph_pipeline,\n",
        "    input_mapping={\n",
        "        \"query\": [\"kg_retriever.query\", \"ranker.query\", \"prompt_builder.query\"]\n",
        "    },\n",
        "    output_mapping={\"generator.replies\": \"replies\"}\n",
        ")\n",
        "\n",
        "web_super = SuperComponent(\n",
        "    pipeline=web_pipeline,\n",
        "    input_mapping={\n",
        "        \"query\": [\"search.query\", \"ranker.query\", \"prompt_builder.query\"]\n",
        "    },\n",
        "    output_mapping={\"generator.replies\": \"replies\"}\n",
        ")\n",
        "\n",
        "# define tools based on the wrapped pipelines\n",
        "embedding_tool = ComponentTool(\n",
        "    name=\"embedding_search\",\n",
        "    component=embedding_super,\n",
        "    description=(\n",
        "        \"Answer questions using information retrieved from an internal document store containing content \"\n",
        "        \"about the European Union’s news, policies, laws, and actions. Answers are based strictly on \"\n",
        "        \"retrieved documents using semantic similarity, with no assumptions. References are included as HTML links.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "graph_tool = ComponentTool(\n",
        "    name=\"graph_search\",\n",
        "    component=graph_super,\n",
        "    description=(\n",
        "        \"Answer questions using structured information from a knowledge graph containing factual relationships \"\n",
        "        \"about the European Union’s news, policies, laws, and actions. The graph includes relationships (triples) \"\n",
        "        \"and the original source documents. Answers are grounded in these facts, with references provided as HTML links.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "web_tool = ComponentTool(\n",
        "    name=\"web_search\",\n",
        "    component=web_super,\n",
        "    description=(\n",
        "        \"Retrieve potentially relevant information from the web. Results are based on live internet content and may \"\n",
        "        \"include a variety of sources. The retrieved information is not guaranteed to be factual. References are provided \"\n",
        "        \"as HTML links using either inferred titles or domain names.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# define the agent's system behavior and reasoning instructions\n",
        "system_prompt = \"\"\"\n",
        "You are a highly intelligent assistant with access to 3 specialized tools for answering questions\n",
        "about the European Union’s news, policies, laws, and actions.\n",
        "\n",
        "You have access to:\n",
        "\n",
        "- embedding_search: Retrieves semantically relevant information from an internal document store.\n",
        "  Answers must be based strictly on the retrieved documents, using inline HTML links for references.\n",
        "\n",
        "- graph_search: Uses a knowledge graph containing factual relationships (triples) and their source documents.\n",
        "  Answers should be grounded in these structured relationships, using HTML links for citations.\n",
        "\n",
        "- web_search: Retrieves the most recent and relevant information from the web.\n",
        "  Answers should reflect real-time sources, with references using HTML links. If no title is available,\n",
        "  use the domain name of the URL as the anchor text.\n",
        "\n",
        "Your task:\n",
        "1. Use all three tools to answer the user's query.\n",
        "2. Combine insights from embedding_search and graph_search tools to create a complete and informative response in the Internal Search Answer section.\n",
        "3. Provide separetely inshights from web_search tool to complete the informative response in the Web Search Insights section.\n",
        "3. In each sentece of your answer add the references you were based on.\n",
        "4. Ensure all references are included as inline HTML anchor tags, using titles or domain names as specified.\n",
        "5. If there is a conflict between the information retrieved from the Web Search and the other tools, highlight the discrepancy separetely if there is one in the Conflicts for Internal and Web Search section.\n",
        "6. For any part of the answer generated from web_search too, always clearly indicate that the information comes from the web.\n",
        "7. Output the answer in a structured markdown format.\n",
        "8. Use bullet lists whenever it makes sense.\n",
        "9. Do not add a references section at the end of the answer, just use references within the body of text.\n",
        "\n",
        "Your output should have three sections if there are no conflicts or four sections if there are conflicts:\n",
        "\n",
        "Thought Process:\n",
        "- Describe step-by-step how each tool contributed to your reasoning and answer.\n",
        "\n",
        "Internal Search Answer:\n",
        "- Provide a clear, concise answer supported by insights from embedding_search and graph_search tools, indicating from which tool the answer is based on.\n",
        "\n",
        "Web Search Insights:\n",
        "- Any content derived from a web search must be explicitly identified as such in the response here.\n",
        "\n",
        "Conflicts for Internal and Web Search:\n",
        "- Any conflict of information derived from the internal compared with the web search be explicitly identified as such in the response here.\n",
        "\n",
        "Always include this disclaimer at the end of the final answer:\n",
        "Disclaimer: This is AI generated content — please use it with caution.\n",
        "\"\"\"\n",
        "\n",
        "# create the agent with the toolset and system prompt\n",
        "agent = Agent(\n",
        "    chat_generator=chat_generator,\n",
        "    tools=[embedding_tool, graph_tool, web_tool],\n",
        "    system_prompt=system_prompt\n",
        ")\n",
        "\n",
        "# prepare the agent for interaction\n",
        "agent.warm_up()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKyF8GU31xob"
      },
      "outputs": [],
      "source": [
        "def run_qa_turn(agent, messages, user_input):\n",
        "    \"\"\"\n",
        "    Run a single Q&A turn with the agent.\n",
        "\n",
        "    Appends the user's input to the message history, executes the agent run,\n",
        "    and parses the response into key parts including tool calls, tool outputs,\n",
        "    and the final answer.\n",
        "\n",
        "    Args:\n",
        "        agent (Agent): The config multi-tool agent.\n",
        "        messages (List[ChatMessage]): Conversation history.\n",
        "        user_input (str): The current user input.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[ChatMessage], Dict]: A tuple containing the updated messages list,\n",
        "        and a dictionary with:\n",
        "            - user_input: the last user message as a string\n",
        "            - tool_calls: list of ToolCall objects\n",
        "            - tool_results: mapping of tool names to stringified output\n",
        "            - final_answer: the assistant’s concluding response\n",
        "    \"\"\"\n",
        "    # ddd user input to message history and execute the agent pipeline\n",
        "    messages.append(ChatMessage.from_user(user_input))\n",
        "\n",
        "    result = agent.run(messages=messages, max_steps=10)\n",
        "    messages = result[\"messages\"]\n",
        "\n",
        "    # init output containers\n",
        "    tool_calls = []\n",
        "    tool_results = {}\n",
        "    final_answer = None\n",
        "    last_user_input = None\n",
        "\n",
        "    # parse returned messages\n",
        "    for msg in messages:\n",
        "        role = msg._role.value.lower()\n",
        "        content = msg._content\n",
        "\n",
        "        if role == \"user\" and content and isinstance(content[0], TextContent):\n",
        "            last_user_input = content[0].text\n",
        "\n",
        "        elif role == \"assistant\" and content:\n",
        "            if isinstance(content[0], ToolCall):\n",
        "                tool_calls.extend(content)\n",
        "            elif isinstance(content[0], TextContent):\n",
        "                final_answer = content[0].text\n",
        "\n",
        "        elif role == \"tool\" and content:\n",
        "            for tool_result in content:\n",
        "                if isinstance(tool_result, ToolCallResult):\n",
        "                    tool_name = tool_result.origin.tool_name\n",
        "                    raw = tool_result.result\n",
        "\n",
        "                    try:\n",
        "                        parsed = ast.literal_eval(raw)\n",
        "\n",
        "                        if isinstance(parsed, dict) and \"replies\" in parsed:\n",
        "                            replies = parsed[\"replies\"]\n",
        "                            if isinstance(replies, list):\n",
        "                                reply_text = \"\\n\".join(replies)\n",
        "                            else:\n",
        "                                reply_text = str(replies)\n",
        "                        else:\n",
        "                            reply_text = str(parsed)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        reply_text = raw  # fallback\n",
        "                        print(f\"Failed to parse tool result for {tool_name}: {e}\")\n",
        "\n",
        "                    tool_results[tool_name] = reply_text\n",
        "\n",
        "    response = {\n",
        "        \"user_input\": last_user_input,\n",
        "        \"tool_calls\": tool_calls,\n",
        "        \"tool_results\": tool_results,\n",
        "        \"final_answer\": final_answer\n",
        "    }\n",
        "\n",
        "    return messages, response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7J7zG4YQDyq"
      },
      "source": [
        "#### Fast API and UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bPOEPSjqpCN"
      },
      "outputs": [],
      "source": [
        "# init fastapi app\n",
        "app = FastAPI()\n",
        "\n",
        "# mount the static directory\n",
        "app.mount(\"/static\", StaticFiles(directory=\"/content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/static\"), name=\"static\")\n",
        "\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    \"\"\"Request model for pipeline-based Q&A.\"\"\"\n",
        "    question: str\n",
        "    pipeline: str\n",
        "\n",
        "\n",
        "class AgentChatRequest(BaseModel):\n",
        "    \"\"\"Request model for agent-based Q&A.\"\"\"\n",
        "    question: str\n",
        "\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def root_chat():\n",
        "    \"\"\"\n",
        "    Serve the main chat tab HTML page.\n",
        "\n",
        "    Returns:\n",
        "        HTMLResponse: Rendered HTML or 404 error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(\"/content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/templates/chat_tab.html\", \"r\") as file:\n",
        "            return HTMLResponse(file.read())\n",
        "    except FileNotFoundError:\n",
        "        return HTMLResponse(\"Chat tab not found\", status_code=404)\n",
        "\n",
        "\n",
        "@app.get(\"/graphrag\", response_class=HTMLResponse)\n",
        "async def graphrag_page():\n",
        "    \"\"\"\n",
        "    Serve the GraphRAG tab HTML page.\n",
        "\n",
        "    Returns:\n",
        "        HTMLResponse: Rendered HTML or 404 error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(\"/content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/templates/graphrag_tab.html\", \"r\") as file:\n",
        "            return HTMLResponse(file.read())\n",
        "    except FileNotFoundError:\n",
        "        return HTMLResponse(\"GraphRAG tab not found\", status_code=404)\n",
        "\n",
        "\n",
        "@app.get(\"/visualgraph\", response_class=HTMLResponse)\n",
        "async def visualgraph_page():\n",
        "    \"\"\"\n",
        "    Serve the visual graph tab HTML page.\n",
        "\n",
        "    Returns:\n",
        "        HTMLResponse: Rendered HTML or 404 error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(\"/content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/templates/visualgraph_tab.html\", \"r\") as file:\n",
        "            return HTMLResponse(file.read())\n",
        "    except FileNotFoundError:\n",
        "        return HTMLResponse(\"Visual GraphRAG tab not found\", status_code=404)\n",
        "\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_with_pipeline(data: ChatRequest) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Handle a user query using the selected pipeline.\n",
        "\n",
        "    Args:\n",
        "        data (ChatRequest): Contains user question and selected pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Response from the generator.\n",
        "    \"\"\"\n",
        "    question = data.question.strip()\n",
        "    pipeline = data.pipeline.lower()\n",
        "    try:\n",
        "        if pipeline == \"graph\":\n",
        "          result = graph_pipeline.run({\n",
        "              \"kg_retriever\": {\"query\": question},\n",
        "              \"ranker\": {\"query\": question},\n",
        "              \"prompt_builder\": {\"query\": question}\n",
        "          })\n",
        "        elif pipeline == \"web\":\n",
        "          result = web_pipeline.run({\n",
        "              \"search\": {\"query\": question},\n",
        "              \"ranker\": {\"query\": question},\n",
        "              \"prompt_builder\": {\"query\": question}\n",
        "          })\n",
        "        elif pipeline == \"rag\":\n",
        "          result = emb_pipeline.run({\n",
        "              \"embedder\": {\"text\": question},\n",
        "              \"retriever\": {\"top_k\": 5},\n",
        "              \"reranker\": {\"query\": question},\n",
        "              \"prompt_builder\": {\"query\": question}\n",
        "          })\n",
        "        else:\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid pipeline specified.\")\n",
        "\n",
        "        reply = result[\"generator\"][\"replies\"][0]\n",
        "        return {\"response\": reply}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Processing error: {repr(e)}\")\n",
        "\n",
        "\n",
        "conversation_state = {}\n",
        "\n",
        "\n",
        "@app.post(\"/chat_agent\")\n",
        "async def chat_with_agent(data: AgentChatRequest) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Handle an agent-based Q&A turn.\n",
        "\n",
        "    Args:\n",
        "        data (AgentChatRequest): Contains user question.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Final answer and any tool-specific results.\n",
        "    \"\"\"\n",
        "    question = data.question.strip()\n",
        "\n",
        "    try:\n",
        "        user_id = \"default\" # default mock user\n",
        "        messages = conversation_state.get(user_id, [])\n",
        "\n",
        "        messages, result = run_qa_turn(agent, messages, question)\n",
        "        conversation_state[user_id] = messages\n",
        "\n",
        "        tool_results = {\n",
        "            tool: output for tool, output in result.get(\"tool_results\", {}).items()\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            \"response\": result.get(\"final_answer\", \"No answer generated.\"),\n",
        "            \"tool_results\": tool_results\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Agent error: {repr(e)}\")\n",
        "\n",
        "\n",
        "@app.post(\"/graphrag_explain\")\n",
        "async def graphrag_explain(request: Request):\n",
        "    \"\"\"\n",
        "    Query Neo4j and return matching triples for a given keyword.\n",
        "\n",
        "    Args:\n",
        "        request (Request): Incoming JSON with 'query'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Matching node and edge information.\n",
        "    \"\"\"\n",
        "    data = await request.json()\n",
        "    query = data.get(\"query\", \"\").strip()\n",
        "\n",
        "    if not query:\n",
        "        raise HTTPException(status_code=400, detail=\"Query is required.\")\n",
        "\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            results = session.run(\"\"\"\n",
        "                MATCH (n)-[r]-(connected)\n",
        "                WHERE toLower(n.id) CONTAINS toLower($query)\n",
        "                RETURN n, r, connected\n",
        "            \"\"\", {\"query\": query})\n",
        "\n",
        "            output = []\n",
        "            for record in results:\n",
        "                output.append({\n",
        "                    \"node\": dict(record[\"n\"]),\n",
        "                    \"relation\": dict(record[\"r\"]),\n",
        "                    \"connected\": dict(record[\"connected\"])\n",
        "                })\n",
        "\n",
        "        return {\"results\": output}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"GraphRAG error: {repr(e)}\")\n",
        "\n",
        "\n",
        "@app.post(\"/graphrag_graph_data\")\n",
        "async def graphrag_graph_data(request: Request):\n",
        "    \"\"\"\n",
        "    Return nodes and edges for graph visualization.\n",
        "\n",
        "    Args:\n",
        "        request (Request): JSON body with optional 'query'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Graph data with node and edge lists.\n",
        "    \"\"\"\n",
        "    data = await request.json()\n",
        "    query = data.get(\"query\", \"\").strip()\n",
        "\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            nodes = {}\n",
        "            edges = []\n",
        "\n",
        "            if query:\n",
        "                cypher = \"\"\"\n",
        "                    MATCH (n)-[r]-(connected)\n",
        "                    WHERE toLower(n.id) CONTAINS toLower($query)\n",
        "                    RETURN n, r, connected\n",
        "                \"\"\"\n",
        "                results = session.run(cypher, {\"query\": query})\n",
        "            else:\n",
        "                cypher = \"\"\"\n",
        "                    MATCH (s)-[r]->(t)\n",
        "                    WHERE type(r) <> 'MENTIONS'\n",
        "                    RETURN s, r, t\n",
        "                \"\"\"\n",
        "                results = session.run(cypher)\n",
        "\n",
        "            for record in results:\n",
        "                s_node = record.get(\"n\") or record.get(\"s\")\n",
        "                t_node = record.get(\"connected\") or record.get(\"t\")\n",
        "                r = record[\"r\"]\n",
        "\n",
        "                def build_node(node):\n",
        "                    return {\n",
        "                        \"id\": node[\"id\"],\n",
        "                        \"label\": node.get(\"label\", node[\"id\"]),\n",
        "                        \"type\": list(node.labels)[0] if node.labels else \"default\"\n",
        "                    }\n",
        "\n",
        "                source = build_node(s_node)\n",
        "                target = build_node(t_node)\n",
        "\n",
        "                nodes[source[\"id\"]] = source\n",
        "                nodes[target[\"id\"]] = target\n",
        "\n",
        "                edges.append({\n",
        "                    \"from\": source[\"id\"],\n",
        "                    \"to\": target[\"id\"],\n",
        "                    \"label\": r.type\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"nodes\": list(nodes.values()),\n",
        "            \"edges\": edges\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Graph fetch error: {repr(e)}\")\n",
        "\n",
        "\n",
        "@app.get(\"/{file_path:path}\", response_class=HTMLResponse)\n",
        "async def get_html(file_path: str):\n",
        "    \"\"\"\n",
        "    Fallback route to serve any HTML file from /content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/templates.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Relative path to HTML file.\n",
        "\n",
        "    Returns:\n",
        "        HTMLResponse: Rendered HTML or 404 error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(f\"/content/Hybrid-Multi-Agent-GraphRAG-for-E-Government/templates/{file_path}\", \"r\") as file:\n",
        "            return HTMLResponse(file.read())\n",
        "    except FileNotFoundError:\n",
        "        return HTMLResponse(\"File not found\", status_code=404)\n",
        "\n",
        "\n",
        "def run_app():\n",
        "    \"\"\"\n",
        "    Launch the FastAPI server using Uvicorn.\n",
        "    \"\"\"\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, timeout_keep_alive=600)\n",
        "\n",
        "\n",
        "# start server in a background thread\n",
        "thread = Thread(target=run_app)\n",
        "thread.start()\n",
        "\n",
        "# uutput server url in colab\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifSGHfu-vRJI"
      },
      "source": [
        "#### Faithfulness Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtFhXK-LvRJI"
      },
      "outputs": [],
      "source": [
        "def find_json_block(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Attempt to find the first valid JSON block in a string using regex.\n",
        "    \"\"\"\n",
        "    json_pattern = re.compile(r'{.*}', re.DOTALL)\n",
        "    matches = json_pattern.findall(text)\n",
        "    for match in matches:\n",
        "        try:\n",
        "            json.loads(match)\n",
        "            return match\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return text  # fallback to full text\n",
        "\n",
        "def try_fix_json(json_str: str) -> str:\n",
        "    json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)\n",
        "    if json_str.count('{') > json_str.count('}'):\n",
        "        json_str += \"}\"\n",
        "    if json_str.count('[') > json_str.count(']'):\n",
        "        json_str += \"]\"\n",
        "    return json_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AOK2cwPvRJI"
      },
      "outputs": [],
      "source": [
        "class CustomFaithfulnessEvaluator(Component):\n",
        "    def __init__(self, model=\"gpt-4.1\", instructions: str = None):\n",
        "        super().__init__()\n",
        "        self.chat_generator = OpenAIChatGenerator(model=model)\n",
        "        self.instructions = instructions or (\n",
        "            \"You are evaluating the faithfulness of a predicted answer based on a provided context.\\n\\n\"\n",
        "            \"You will receive:\\n\"\n",
        "            \"- a question\\n\"\n",
        "            \"- a context: a set of retrieved documents or passages used to generate the answer\\n\"\n",
        "            \"- a predicted answer generated based on this context\\n\\n\"\n",
        "            \"TASK:\\n\"\n",
        "            \"1. Break the predicted answer into factual statements. Produce the factual statements solely based on the Predicted Answer.\\n\"\n",
        "            \"2. For each statement:\\n\"\n",
        "            \"   a. If the statement is clearly supported by the context → score = 1\\n\"\n",
        "            \"      justification: describe how the statement is explicitly supported in the context\\n\\n\"\n",
        "            \"   b. If the statement is not supported or the context is silent → score = 0\\n\"\n",
        "            \"      justification: explain the lack of evidence in the context\\n\\n\"\n",
        "            \"   c. If the statement includes or is equivalent to 'Final Answer: inconclusive' AND this is justified by the lack of support in context → score = -1\\n\"\n",
        "            \"      justification: explain that the answer is inconclusive and no factual claims were made\\n\\n\"\n",
        "            \"   d. If the statement includes or is equivalent to 'Final Answer: inconclusive' BUT the context does contain sufficient information to answer the question → score = -2\\n\"\n",
        "            \"      justification: explain that the model incorrectly concluded inconclusive despite having supporting context\\n\\n\"\n",
        "            \"Format your response as JSON:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"statements\": [...],\\n'\n",
        "            '  \"statement_scores\": [...],\\n'\n",
        "            '  \"justifications\": [\\n'\n",
        "            '    \"supported: <details>\",\\n'\n",
        "            '    \"unsupported: <details>\",\\n'\n",
        "            '    \"inconclusive (correct): <details>\",\\n'\n",
        "            '    \"inconclusive (incorrect): <details>\"\\n'\n",
        "            '  ]\\n'\n",
        "            \"}\"\n",
        "        )\n",
        "\n",
        "    @component.output_types(\n",
        "        individual_scores=List[float],\n",
        "        score=float,\n",
        "        results=List[Dict[str, Any]]\n",
        "    )\n",
        "    def run(self, questions: List[str], contexts: List[List[str]], predicted_answers: List[str]) -> Dict[str, Any]:\n",
        "        results = []\n",
        "        individual_scores = []\n",
        "\n",
        "        for question, context_list, predicted_answer in zip(questions, contexts, predicted_answers):\n",
        "            full_context = \" \".join(context_list)\n",
        "\n",
        "            prompt = (\n",
        "                f\"{self.instructions}\\n\\n\"\n",
        "                f\"Question: {question}\\n\\n\"\n",
        "                f\"Context:\\n{full_context}\\n\\n\"\n",
        "                f\"Predicted Answer:\\n{predicted_answer}\\n\\n\"\n",
        "                f\"Respond with a JSON object like this:\\n\"\n",
        "                f'{{\"statements\": [...], \"statement_scores\": [...]}}'\n",
        "            )\n",
        "\n",
        "            parsed = None\n",
        "            retries = 3\n",
        "\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = self.chat_generator.run([ChatMessage.from_user(prompt)])\n",
        "                    reply_text = response[\"replies\"][0].text\n",
        "\n",
        "                    json_candidate = find_json_block(reply_text)\n",
        "\n",
        "                    try:\n",
        "                        parsed = json.loads(json_candidate)\n",
        "                        break\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"[Attempt {attempt+1}] JSON parsing failed — trying to fix...\")\n",
        "                        fixed_text = try_fix_json(json_candidate)\n",
        "                        parsed = json.loads(fixed_text)\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"[Attempt {attempt+1}] Failed: {str(e)}\")\n",
        "                    time.sleep(1)\n",
        "\n",
        "            if parsed is None:\n",
        "                print(\"Failed to get a valid response after retries.\")\n",
        "                parsed = {\"statements\": [], \"statement_scores\": []}\n",
        "\n",
        "            score = float(np.mean(parsed[\"statement_scores\"])) if parsed[\"statement_scores\"] else 0.0\n",
        "            results.append({**parsed, \"score\": score})\n",
        "            individual_scores.append(score)\n",
        "\n",
        "        final_score = float(np.mean(individual_scores)) if individual_scores else 0.0\n",
        "\n",
        "        return {\n",
        "            \"results\": results,\n",
        "            \"individual_scores\": individual_scores,\n",
        "            \"score\": final_score\n",
        "        }\n",
        "\n",
        "standard_evaluator = CustomFaithfulnessEvaluator(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zOWUZrUvRJI"
      },
      "outputs": [],
      "source": [
        "class CustomBasicAgentFaithfulnessEvaluator(Component):\n",
        "    def __init__(self, model=\"gpt-4.1\", instructions: str = None):\n",
        "        super().__init__()\n",
        "        self.chat_generator = OpenAIChatGenerator(model=model)\n",
        "        self.instructions = instructions or (\n",
        "            \"You are evaluating the faithfulness and attribution of a predicted answer.\\n\\n\"\n",
        "            \"You will receive:\\n\"\n",
        "            \"- a question\\n\"\n",
        "            \"- embedding_search_context: information from internal knowledge from embedding_search tool\\n\"\n",
        "            \"- graph_search_context: information from internal knowledge from graph_search tool\\n\"\n",
        "            \"- web_search_context: information retrieved from web_search tools\\n\"\n",
        "            \"- a predicted answer from an agent\\n\\n\"\n",
        "            \"   The predicted answer you will receive should include a Internal Search Answer section produced with embedding_search and graph_search tools results\\n\"\n",
        "            \"   The predicted answer you will receive should include a Web Search Insights section produced with web_search tool results\\n\"\n",
        "            \"TASK:\\n\"\n",
        "            \"1. Break the predicted answer into factual statements. Produce the factual statements solely based on the Predicted Answer.\\n\"\n",
        "            \"2. For each statement:\\n\"\n",
        "            \"   a. If the statement is clearly supported by the context → score = 1\\n\"\n",
        "            \"      justification: describe how the statement is explicitly supported in the context\\n\\n\"\n",
        "            \"   b. If the statement is not supported or the context is silent → score = 0\\n\"\n",
        "            \"      justification: explain the lack of evidence in the context\\n\\n\"\n",
        "            \"   c. If the statement includes or is equivalent to 'Final Answer: inconclusive' AND this is justified by the lack of support in context → score = -1\\n\"\n",
        "            \"      justification: explain that the answer is inconclusive and no factual claims were made\\n\\n\"\n",
        "            \"   d. If the statement includes or is equivalent to 'Final Answer: inconclusive' BUT the context does contain sufficient information to answer the question → score = -2\\n\"\n",
        "            \"      justification: explain that the model incorrectly concluded inconclusive despite having supporting context\\n\\n\"\n",
        "            \"Format your response as JSON:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"statements\": [...],\\n'\n",
        "            '  \"statement_scores\": [...],\\n'\n",
        "            '  \"justifications\": [\\n'\n",
        "            '    \"supported: <details>\",\\n'\n",
        "            '    \"unsupported: <details>\",\\n'\n",
        "            '    \"inconclusive (correct): <details>\",\\n'\n",
        "            '    \"inconclusive (incorrect): <details>\"\\n'\n",
        "            '  ]\\n'\n",
        "            \"}\"\n",
        "        )\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        emb_contexts: List[List[str]],\n",
        "        graph_contexts: List[List[str]],\n",
        "        web_search_contexts: List[List[str]],\n",
        "        predicted_answers: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        results = []\n",
        "\n",
        "        for question, emb_retrieved, graph_retrieved, web_search, answer in zip(\n",
        "            questions, emb_contexts, graph_contexts, web_search_contexts, predicted_answers\n",
        "        ):\n",
        "            full_emb = \" \".join(emb_retrieved)\n",
        "            full_graph = \" \".join(graph_retrieved)\n",
        "            full_web = \" \".join(web_search)\n",
        "\n",
        "            if \"final answer: inconclusive\" in answer.lower():\n",
        "                results.append({\n",
        "                    \"statements\": [],\n",
        "                    \"statement_scores\": [],\n",
        "                    \"justifications\": []\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            prompt = (\n",
        "                f\"{self.instructions}\\n\\n\"\n",
        "                f\"Question:\\n{question}\\n\\n\"\n",
        "                f\"Contexts:\\n\"\n",
        "                f\"embedding_search_context:\\n{full_emb}\\n\\n\"\n",
        "                f\"graph_search_context:\\n{full_graph}\\n\\n\"\n",
        "                f\"web_search_context:\\n{full_web}\\n\\n\"\n",
        "                f\"Predicted Answer:\\n{answer}\"\n",
        "            )\n",
        "\n",
        "            parsed = None\n",
        "            retries = 3\n",
        "\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = self.chat_generator.run([ChatMessage.from_user(prompt)])\n",
        "                    reply_text = response[\"replies\"][0].text\n",
        "\n",
        "                    json_candidate = find_json_block(reply_text)\n",
        "\n",
        "                    try:\n",
        "                        parsed = json.loads(json_candidate)\n",
        "                        break\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(f\"Attempt {attempt+1}: JSON parsing failed — trying to fix...\")\n",
        "                        fixed_text = try_fix_json(json_candidate)\n",
        "                        parsed = json.loads(fixed_text)\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
        "                    time.sleep(1)\n",
        "\n",
        "            if parsed is None:\n",
        "                print(\"Failed to get valid response after retries.\")\n",
        "                parsed = {\n",
        "                    \"statements\": [],\n",
        "                    \"statement_scores\": [],\n",
        "                    \"justifications\": []\n",
        "                }\n",
        "\n",
        "            results.append(parsed)\n",
        "\n",
        "        return {\n",
        "            \"results\": results\n",
        "        }\n",
        "\n",
        "# instantiate the evaluator\n",
        "agent_evaluator = CustomBasicAgentFaithfulnessEvaluator(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khW7v8CYvRJI"
      },
      "outputs": [],
      "source": [
        "# agentic pipeline evaluation\n",
        "\n",
        "results_file = \"agent_basic_faithfulness_evaluation_results.json\"\n",
        "\n",
        "if os.path.exists(results_file):\n",
        "    with open(results_file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "else:\n",
        "    results = []\n",
        "\n",
        "with open(\"eval/eval_questions.json\", \"r\") as f: # add the questions in the repository (json format)\n",
        "    data = json.load(f)\n",
        "    questions = data.get(\"questions\", [])\n",
        "\n",
        "processed_questions = {entry[\"question\"] for entry in results if \"question\" in entry}\n",
        "\n",
        "for question in questions:\n",
        "    if question in processed_questions:\n",
        "        print(f\"Skipping already processed question: {question}\")\n",
        "        continue\n",
        "\n",
        "    conversation_state = {}\n",
        "    user_id = \"default\"\n",
        "    print(f\"Processing: {question}\")\n",
        "    messages = conversation_state.get(user_id, [])\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        messages, result = run_qa_turn(agent, messages, question)\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        conversation_state[user_id] = messages\n",
        "\n",
        "        tool_results = result.get(\"tool_results\", {})\n",
        "        answer = result.get(\"final_answer\", \"No answer generated.\")\n",
        "        if isinstance(answer, tuple):\n",
        "            answer = answer[0]\n",
        "\n",
        "        emb_context = tool_results.get(\"embedding_search\", [])\n",
        "        if not isinstance(emb_context, list):\n",
        "            emb_context = [emb_context]\n",
        "\n",
        "        graph_context = tool_results.get(\"graph_search\", [])\n",
        "        if not isinstance(graph_context, list):\n",
        "            graph_context = [graph_context]\n",
        "\n",
        "        web_context = tool_results.get(\"web_search\", [])\n",
        "        if not isinstance(web_context, list):\n",
        "            web_context = [web_context]\n",
        "\n",
        "\n",
        "        faithfulness_result = agent_evaluator.run(\n",
        "            questions=[question],\n",
        "            emb_contexts=[emb_context],\n",
        "            graph_contexts=[graph_context],\n",
        "            web_search_contexts=[web_context],\n",
        "            predicted_answers=[answer]\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"final_answer\": answer,\n",
        "            \"emb_based_context\": emb_context,\n",
        "            \"graph_based_context\": graph_context,\n",
        "            \"web_based_context\": web_context,\n",
        "            \"evaluation\": faithfulness_result,\n",
        "            \"latency_seconds\": latency\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3sxIigFvRJI"
      },
      "outputs": [],
      "source": [
        "# embeddings pipeline evaluation\n",
        "results_file = \"qa_emb_pipeline_evaluation_results.json\"\n",
        "\n",
        "# load the the questions\n",
        "with open(\"eval/eval_questions.json\", \"r\") as f:  # add the questions in the repository (json format)\n",
        "    data = json.load(f)\n",
        "    questions = data.get(\"questions\", [])\n",
        "\n",
        "# init or load previous results\n",
        "if os.path.exists(results_file):\n",
        "    with open(results_file, \"r\") as f:\n",
        "        saved_results = json.load(f)\n",
        "else:\n",
        "    saved_results = [{} for _ in questions]\n",
        "\n",
        "while len(saved_results) < len(questions):\n",
        "    saved_results.append({})\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "    existing_entry = saved_results[idx]\n",
        "\n",
        "    # only in rerun: skip if already successfully processed (no error and has matching question)\n",
        "    if existing_entry.get(\"question\") == question and \"error\" not in existing_entry:\n",
        "        print(f\"Skipping already processed question: {question}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing question: {question}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        result = emb_pipeline.run({\n",
        "            \"embedder\": {\"text\": question},\n",
        "            \"retriever\": {\"top_k\": 5},\n",
        "            \"reranker\": {\"query\": question},\n",
        "            \"prompt_builder\": {\"query\": question}\n",
        "        })\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        generated_answer = result[\"generator\"][\"replies\"][0]\n",
        "        contexts = [doc.content for doc in result[\"output_docs\"][\"documents\"]]\n",
        "        contexts_for_eval = [contexts]\n",
        "\n",
        "        faithfulness_result = standard_evaluator.run(\n",
        "            questions=[question],\n",
        "            contexts=contexts_for_eval,\n",
        "            predicted_answers=[generated_answer]\n",
        "        )\n",
        "\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"generated_answer\": generated_answer,\n",
        "            \"contexts\": contexts,\n",
        "            \"results\": faithfulness_result,\n",
        "            \"latency_seconds\": latency,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        print(f\"Error processing question: {question}\")\n",
        "        print(str(e))\n",
        "\n",
        "    saved_results[idx] = entry\n",
        "\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(saved_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4oB0MlLvRJI"
      },
      "outputs": [],
      "source": [
        "# graphrag pipeline evalulation\n",
        "\n",
        "results_file = \"qa_graph_pipeline_evaluation_results.json\"\n",
        "\n",
        "# load the the questions\n",
        "with open(\"eval/eval_questions.json\", \"r\") as f:  # add the questions in the repository (json format)\n",
        "    data = json.load(f)\n",
        "    questions = data.get(\"questions\", [])\n",
        "\n",
        "# init or load previous results\n",
        "if os.path.exists(results_file):\n",
        "    with open(results_file, \"r\") as f:\n",
        "        saved_results = json.load(f)\n",
        "else:\n",
        "    saved_results = [{} for _ in questions]\n",
        "\n",
        "while len(saved_results) < len(questions):\n",
        "    saved_results.append({})\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "    existing_entry = saved_results[idx]\n",
        "\n",
        "    # only in rerun: skip if already successfully processed (no error and has matching question)\n",
        "    if existing_entry.get(\"question\") == question and \"error\" not in existing_entry:\n",
        "        print(f\"Skipping already processed question: {question}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing question: {question}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        result = graph_pipeline.run({\n",
        "            \"kg_retriever\": {\"query\": question},\n",
        "            \"ranker\": {\"query\": question},\n",
        "            \"prompt_builder\": {\"query\": question}\n",
        "        })\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        generated_answer = result[\"generator\"][\"replies\"][0]\n",
        "        contexts = [doc.content for doc in result[\"output_docs\"][\"documents\"]]\n",
        "        contexts_for_eval = [contexts]\n",
        "\n",
        "        faithfulness_result = standard_evaluator.run(\n",
        "            questions=[question],\n",
        "            contexts=contexts_for_eval,\n",
        "            predicted_answers=[generated_answer]\n",
        "        )\n",
        "\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"generated_answer\": generated_answer,\n",
        "            \"contexts\": contexts,\n",
        "            \"results\": faithfulness_result,\n",
        "            \"latency_seconds\": latency,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        print(f\"Error processing question: {question}\")\n",
        "        print(str(e))\n",
        "\n",
        "    saved_results[idx] = entry\n",
        "\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(saved_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZIaIcX2vRJI"
      },
      "outputs": [],
      "source": [
        "# web search pipeline evalulation\n",
        "\n",
        "results_file = \"qa_web_pipeline_evaluation_results.json\"\n",
        "\n",
        "# load the the questions\n",
        "with open(\"eval/eval_questions.json\", \"r\") as f:  # add the questions in the repository (json format)\n",
        "    data = json.load(f)\n",
        "    questions = data.get(\"questions\", [])\n",
        "\n",
        "# init or load previous results\n",
        "if os.path.exists(results_file):\n",
        "    with open(results_file, \"r\") as f:\n",
        "        saved_results = json.load(f)\n",
        "else:\n",
        "    saved_results = [{} for _ in questions]\n",
        "\n",
        "while len(saved_results) < len(questions):\n",
        "    saved_results.append({})\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "    existing_entry = saved_results[idx]\n",
        "\n",
        "    # only in rerun: skip if already successfully processed (no error and has matching question)\n",
        "    if existing_entry.get(\"question\") == question and \"error\" not in existing_entry:\n",
        "        print(f\"Skipping already processed question: {question}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing question: {question}\")\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        result = web_pipeline.run({\n",
        "            \"search\": {\"query\": question},\n",
        "            \"ranker\": {\"query\": question},\n",
        "            \"prompt_builder\": {\"query\": question}\n",
        "        })\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "        generated_answer = result[\"generator\"][\"replies\"][0]\n",
        "        contexts = [doc.content if doc.content is not None else \"\" for doc in result[\"output_docs\"][\"documents\"]]\n",
        "        contexts_for_eval = [contexts]\n",
        "\n",
        "        faithfulness_result = standard_evaluator.run(\n",
        "            questions=[question],\n",
        "            contexts=contexts_for_eval,\n",
        "            predicted_answers=[generated_answer]\n",
        "        )\n",
        "\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"generated_answer\": generated_answer,\n",
        "            \"contexts\": contexts,\n",
        "            \"results\": faithfulness_result,\n",
        "            \"latency_seconds\": latency,\n",
        "\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        entry = {\n",
        "            \"question\": question,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        print(f\"Error processing question: {question}\")\n",
        "        print(str(e))\n",
        "\n",
        "    saved_results[idx] = entry\n",
        "\n",
        "    with open(results_file, \"w\") as f:\n",
        "        json.dump(saved_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWX_aMamDFAh"
      },
      "source": [
        "#### Exaplanatory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlLTGzGIhbyw"
      },
      "outputs": [],
      "source": [
        "def print_relationship_schema(record):\n",
        "    \"\"\"\n",
        "    Print the relationship between two nodes in a readable schema format.\n",
        "\n",
        "    Args:\n",
        "        record (neo4j.Record): A Neo4j record containing 'n', 'r', and 'connected' keys.\n",
        "    \"\"\"\n",
        "    n = record[\"n\"]\n",
        "    r = record[\"r\"]\n",
        "    connected = record[\"connected\"]\n",
        "\n",
        "    n_label = list(n.labels)[0] if n.labels else \"Entity\"\n",
        "    connected_label = list(connected.labels)[0] if connected.labels else \"Entity\"\n",
        "\n",
        "    print(f\"({n_label}: {n['id']}) -[{r.type}]-> ({connected_label}: {connected['id']})\")\n",
        "\n",
        "\n",
        "# query and print relationships for nodes matching the search term\n",
        "search_term = \"clean industrial deal\"\n",
        "\n",
        "with driver.session() as session:\n",
        "    results = session.run(\"\"\"\n",
        "        MATCH (n)-[r]-(connected)\n",
        "        WHERE toLower(n.id) CONTAINS toLower($query)\n",
        "        RETURN n, r, connected\n",
        "    \"\"\", {\"query\": search_term})\n",
        "\n",
        "    for record in results:\n",
        "        print_relationship_schema(record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wtGtxTXDRDR"
      },
      "outputs": [],
      "source": [
        "# enable interactive graph widget rendering in colab\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "\n",
        "def visualize_graph_from_search(query: str = None):\n",
        "    \"\"\"\n",
        "    Visualize a subgraph from the Neo4j database.\n",
        "\n",
        "    If a query is provided, performs a fuzzy search on node IDs and visualizes\n",
        "    the resulting subgraph. If no query is provided, visualizes a default larger\n",
        "    subgraph (excluding MENTIONS relationships).\n",
        "\n",
        "    Args:\n",
        "        query (str, optional): A partial or full node ID to filter the graph. Defaults to None.\n",
        "    \"\"\"\n",
        "    driver = GraphDatabase.driver(\n",
        "        uri=os.environ[\"NEO4J_URI\"],\n",
        "        auth=(os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
        "    )\n",
        "\n",
        "    with driver.session() as session:\n",
        "        if query and query.strip():\n",
        "            cypher = \"\"\"\n",
        "                MATCH (n)-[r]-(connected)\n",
        "                WHERE toLower(n.id) CONTAINS toLower($query)\n",
        "                RETURN n, r, connected\n",
        "            \"\"\"\n",
        "            result = session.run(cypher, {\"query\": query})\n",
        "        else:\n",
        "            cypher = \"\"\"\n",
        "                MATCH (s)-[r]->(t)\n",
        "                WHERE type(r) <> 'MENTIONS'\n",
        "                RETURN s, r, t LIMIT 1000\n",
        "            \"\"\"\n",
        "            result = session.run(cypher)\n",
        "\n",
        "        graph = result.graph()\n",
        "\n",
        "    widget = GraphWidget(graph=graph)\n",
        "    widget.node_label_mapping = 'id'\n",
        "    display(widget)\n",
        "\n",
        "\n",
        "# run graph visualization (default)\n",
        "visualize_graph_from_search(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K75VTfzQTGYm"
      },
      "source": [
        "##### Test Default pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6afJHOLTbV1"
      },
      "outputs": [],
      "source": [
        "# test custom knowledge graph retriever\n",
        "kg_retriever = KnowledgeGraphRetriever(\n",
        "    neo4j_uri=NEO4J_URI,\n",
        "    neo4j_user=NEO4J_USER,\n",
        "    neo4j_pass=NEO4J_PASS\n",
        ")\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"kg_search\", kg_retriever)\n",
        "\n",
        "result = pipe.run({\"kg_search\": {\"query\": \"Tell me about the Clean Indrustrial Deal\"}})\n",
        "print(result[\"kg_search\"][\"documents\"][0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxXT6xQrTJMs"
      },
      "outputs": [],
      "source": [
        "# test embeddings pipeline\n",
        "question = \"Tell me about the Clean Indrustrial Deal\"\n",
        "\n",
        "result = emb_pipeline.run({\n",
        "    \"embedder\": {\"text\": question},\n",
        "    \"retriever\": {\"top_k\": 5},\n",
        "    \"reranker\": {\"query\": question},\n",
        "    \"prompt_builder\": {\"query\": question}\n",
        "})\n",
        "\n",
        "print(result[\"generator\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1DA4DQPTl7z"
      },
      "outputs": [],
      "source": [
        "# test web search Pipeline\n",
        "query = \"Tell me about the Clean Indrustrial Deal\"\n",
        "\n",
        "result = web_pipeline.run({\n",
        "    \"search\": {\"query\": query},\n",
        "    \"ranker\": {\"query\": query},\n",
        "    \"prompt_builder\": {\"query\": query}\n",
        "})\n",
        "\n",
        "print(result[\"generator\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfrCedbUYdSA"
      },
      "outputs": [],
      "source": [
        "# test graph search\n",
        "question = \"Tell me about the Clean Indrustrial Deal\"\n",
        "\n",
        "result = graph_pipeline.run({\n",
        "    \"kg_retriever\": {\"query\": question},\n",
        "    \"ranker\": {\"query\": question},\n",
        "    \"prompt_builder\": {\"query\": question}\n",
        "})\n",
        "\n",
        "print(result[\"generator\"][\"replies\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwynE2iar-y6"
      },
      "outputs": [],
      "source": [
        "# test the agents\n",
        "# start the conversation with an initial question\n",
        "messages = []\n",
        "messages, result = run_qa_turn(agent, messages, \"Tell me about the Clean Indrustrial Deal\")\n",
        "\n",
        "print(\"User Input:\", result[\"user_input\"])\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "print(\"Tool Results:\")\n",
        "for tool, out in result[\"tool_results\"].items():\n",
        "    print(f\"{tool}: {out}\")\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "print(\"Final Answer:\", result[\"final_answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iinVXiosGaV"
      },
      "outputs": [],
      "source": [
        "# make a follow up question\n",
        "messages, followup = run_qa_turn(agent, messages, \"Ok, can you provide a shorter answer in less than 100 words?\")\n",
        "print(\"User Input:\", followup[\"user_input\"])\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "print(\"Tool Results:\")\n",
        "for tool, out in followup[\"tool_results\"].items():\n",
        "    print(f\"{tool}: {out}\")\n",
        "print(\"-----------------------------------------------------------------------------------------------\")\n",
        "print(\"Final Answer:\", followup[\"final_answer\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rjaZw4RnWKsL",
        "CIAcD3G4C5xM",
        "giPAmjbuC_om",
        "vlBA-fAZDKtu",
        "kyJjq0u0QJt0",
        "H7J7zG4YQDyq",
        "FWX_aMamDFAh"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
